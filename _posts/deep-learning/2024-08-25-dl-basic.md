---
layout: single
title: "DL"
categories: Deep-Learning
tag: [machine-learning, deep-learning]
toc: true # 목차 보여주기
author_profile: false   # 프로필 제거
# sidebar:    # 프로필 제거 후 사이드바 보여주기
#     nav: "counts"
typora-root-url: ../
---

# ※ 딥러닝의 정의
deep learning = machine learning + deep neural network
e.g. CNN, RNN

DL이 ML보다 좋은 이유는 성능이 좋아서. > feature extraction 때문에 성능이 좋음
ML - 전문가가 feature engineering을 함
DL - feature engineering을 model이 직접 함 (feature extraction)

주의해야 할 점
1. DL은 ML보다 더 많은 학습데이터를 필요로 함
2. DL은 ML보다 더 많은 computing resource를 필요로 함

제안
이미지, 텍스트, 음성 데이터 분석을 하는 경우에는 DL 추천

# ※ Perceptron

## deep neural network
- hidden layer가 2개 이상인 것


# perceptron
- artificial neuron

# multi layer perceptron (MLP)
- perceptron을 여러 층 쌓은 것
- non-linear classifier
- f: activation function (non-linearity)
  - e.g. sigmoid function

# feed-forward
- inference
- 앞으로 데이터를 전달해 주는것

ReLU (Recified Linear Unit)

# output / loss function
loss: target과 predict value의 차이

## cross entropy fuction
정보의 차이가 크면 좋음
두 차이가 극명하게 나뉘는 걸 선호
-(ylogp + (1-y)log(1-p))

# backpropagation
- error backpropagation
- loss 값이 전달됨

## stochastic gradient descent
- 데이터의 일부만 사용 (mini-batch)
- 메모리 최대 활용

## gradient descent
- 데이터 전부 사용 (full-batch)
- 메모리가 많이 필요

e.g. N = 10000개
batch_size = 100
1 epoch = 100 iterations (=wight update 횟수)
1 epoch (full-batch일 때)

mini batch일 때는 epoch * batch_size 만큼 업데이트
full batch일 때는 epoch 만큼만 업데이트

batch size는 일반적으로 2^m을 사용 (gpu의 가성비를 높여주는 숫자)


==============================

# 딥러닝 워크플로우
TensorFlow

==============================


## Residual Block

## transfer learning 전이학습
ResNet34,ResNet50,ResNet101,ResNet152 : pretraind model

# ResNet
gradient update가 잘 안되는 문제를 해결 (gredient vanishing)

gradient 정보를 앞쪽까지 잘 전달해 주자.
loss를 하나는 레이어를 지나고,,, 하나는 살아서 그냥 넘기고,,,  f(x)+x
