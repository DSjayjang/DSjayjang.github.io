---
layout: single
title: "Artificial Neural Network (인공 신경망)"
categories: Deep-Learning
tag: [machine-learning, deep-learning, artificial-neural-network, ann, neural-network, back-propagation, forward-propagation]
toc: true # 목차 보여주기
author_profile: false   # 프로필 제거
# sidebar:    # 프로필 제거 후 사이드바 보여주기
#     nav: "counts"
typora-root-url: ../
---

# ※ Artificial Neural Network

<br>

## ■ Forward Propagation (순전파)
- 입력층 - 은닉층 - 출력층 순서대로 흘러가는 것

<br>

## ■ Back Propagation (역전파)
- Update weights recursively
- 다층 퍼셉트론(MLP)에서 최적값을 찾아가는 과정
- 출력층 - 은닉층 - 입력층 순서대로 반대로 거슬러 올라가는 것

<br>

### 1. 가중치 초기화

<br>

### 2. Forward Propagation
- **입력층에서 은닉층으로의 전파**
- e.g.

$$
\bold W_{1}^{T} \bold x + \bold b_{1} = \bold g
$$

<br>

$$
\bold W_{1}^{T} \bold x + \bold b_{1} = \begin{pmatrix}w_{11}&w_{21}&w_{31}\\w_{12}&w_{22}&w_{32}\\ \end{pmatrix} \begin{pmatrix}x_{1}\\x_{2}\\x_{3}\end{pmatrix} + \begin{pmatrix}b_{1}\\b_{2}\end{pmatrix}
$$

<br>

- **Activation Function을 이용하여 nonlinear하게 만들기**
- e.g.

$$
h_{1} = \phi(g_{1}) \\ h_{2} = \phi(g_{2})
$$

<br>

- **은닉층에서 출력층으로의 전파**

$$
\bold W_{2}^{T} \bold h + \bold b_{2} = \bold z
$$

<br>

- Activation Function을 이용하여 nonlinear하게 만들기
- $y_{1} = \phi(z_{1})$, $y_{2} = \phi(z_{2})$, ...

<br>

### 3. cost function 정의 및 1차 미분식 구하기

<br>

### 4. back propagation을 통한 1차 미분값 구하기

<br>

### 5. parameter 업데이트

