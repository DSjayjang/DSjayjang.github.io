---
layout: single
title: "Convolutional Neural Network (CNN, 합성곱 신경망)"
categories: Deep-Learning
tag: [machine-learning, deep-learning, convolutional-neural-network, cnn]
toc: true # 목차 보여주기
author_profile: false   # 프로필 제거
# sidebar:    # 프로필 제거 후 사이드바 보여주기
#     nav: "counts"
typora-root-url: ../
---

# ※ Convolutional Neural Network
- 이미지 처리와 패턴 인식에 탁월한 성능을 보여주는 신경망
- 이미지 데이터의 공간적 특징을 추출하여 학습하고 이를 기반으로 패턴을 인식하는데 사용
- 구성
  - Convolution Layer (합성곱 층)
  - Pooling Layer (풀링 층)
  - Fully-Connected Layer (완전하게 연결된 층)
- 전체적인 절차
  1. Input (이미지)
  2. Convolution Layer에서 Filter를 통해 Feature map(Channel) 생성
  3. Activation Function을 사용하여 non-linear하게 만듦
  4. Pooling을 통해 Feature map 크기를 줄임
  5. 2~4 반복 : 1 Convolution (또는 1 Set)
  6. Flatten
  7. NN

![1]({{site.url}}/images/dl/cnn/1.jpg)



<br>

## ■ Convolution Layer (합성곱 층)

### □ Filter (Kernel or Mask)
- 이미지를 스캔하기 위한 가중치들의 집합
- 가중치: CNN에서는 학습해야 할 가중치를 '가로세로 넓이를 갖는 하나의 작은 면 형태의 세트'로 구성됨
- kernel의 사이즈가 작으면 디테일한 feature도 처리할수있게 되지만 연산량 증가해서 적정 값을 찾는게 중요함

<br>

### □ Feature Map (특성 시트 / Activation Map)
- Filter로 연산된 결과 (필터 개수만큼 생성됨)
- Filter가 읽어낸 특성값들을 나열하면 다시 가로세로의 면이 된 것
- e.g. 3x3 filter =$\sum_{i,j} w_{ij} \times x_{ij}$
- non-linear activation function으로는 보통 ReLU 사용

<br>

![2]({{site.url}}/images/dl/cnn/2.jpg)

<br>

### □ Stride (보폭)
- 합성곱 층의 출력(feature map)의 크기를 조절하는 요소
- filter를 한번에 이동시키는 값
  - e.g. stride = 1 : filter를 한번에 한칸씩 이동시킴

<br>

### □ Padding (패딩)
- input, output의 사이즈를 같게 하기 위한 방법
- convolution에서 출력되는 사이즈를 줄이지 않기 위해 input 테두리에 0을 채움 (zero padding이라고 함)

<br>

## ■ Pooling Layer (풀링 층)

### □ Pooling (풀링)
- 합성곱 층에서 추출된 feature map의 크기를 줄이는 역할
  - Max Pooling: 주어진 풀링 윈도우 내의 값의 최대값 리턴
  - Mean Pooling: 주어진 풀링 윈도우 내의 값의 평균값 리턴
- 풀링층을 통해 공간크기를 줄이고 계산비용을 줄이면서 특징 보존
- 정보 압축

<br>

## ■ Fully-Connected Layer (완전하게 연결된 층)

### □ Flattening
- 2D array를 한줄로 펼침
- Convolution을 통해 출력된 output을, ANN을 수행하기 위한 input으로 만들기 위해 벡터로 변환 (ANN의 feauture 노드의 개수는 Convolution을 통해 나온 사이즈가 됨)

### □ Artificial Neural Network
- Flattening을 통해 feature들을 입력으로 받고, ANN 수행

<br>

## ■ CNN 전체 과정 요약
- input -> kernel을 이용하여 convolution -> non-linear activation function을 이용하여 비선형성 -> pooling을 통하여 정보 압축
- 위 과정이 1 Convolution (또는 1 Set), 이것을 반복
- 최종적으로 flattening을 통해 벡터로 만든 후, ANN 수행

<br>

## ■ CNN 전체 과정 예시
0. Input: 28x28x1 (1개의 channel)
<br>
-----------1st Convolution------------
<br>

1. Kernel
   - size: 3x3 (실제로는 3x3x1)
   - kernel 개수: 32개
   - stride: 1
   - zero padding 적용
2. Output: 28x28x32 (32개의 channel)
3. Non-linear activation function 적용
4. Pooling
   - size: 2x2
   - max pooling 적용
5. Output: 14x14x32

<br>
-----------2nd Convolution------------
<br>

0. Input: 14x14x32
1. Kernel
   - size: 3x3 (실제로는 3x3x32)
   - kernel 개수: 64개
   - stride: 1
   - zero padding 적용
2. Output: 14x14x64 (64개의 channel)
3. Non-linear activation function 적용
4. Pooling
   - size: 2x2
   - max pooling 적용
5. Output: 7x7x64

<br>
-----------------------
<br>

1. Flattening을 통해 convolution에서 나온 output을 벡터로 변환 (feature의 개수는 7x7x64개)
2. ANN 수행